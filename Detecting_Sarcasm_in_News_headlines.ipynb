{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting Sarcasm in News headlines.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSololNMRfaDPTtvPlWWwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShashwatVv/DetectingSaracasm-In-NewsHeadlines/blob/main/Detecting_Sarcasm_in_News_headlines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Let's first load the dataset. "
      ],
      "metadata": {
        "id": "lRumd9Xv-mQQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYj4fcy9-Tm-",
        "outputId": "9966cd7b-4078-40fc-a2d9-7c799366f374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-28 10:12:08--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.218.128, 108.177.11.128, 74.125.31.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.218.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5643545 (5.4M) [application/json]\n",
            "Saving to: ‘/tmp/sarcasm.json’\n",
            "\n",
            "\r/tmp/sarcasm.json     0%[                    ]       0  --.-KB/s               \r/tmp/sarcasm.json   100%[===================>]   5.38M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-03-28 10:12:08 (218 MB/s) - ‘/tmp/sarcasm.json’ saved [5643545/5643545]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  The libraries to be imported\n",
        "import numpy  as np\n",
        "import json \n",
        "import io\n",
        "import tensorflow  as  tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  as TKN\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences as padseq\n",
        "print(\"Imported!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rol6Vk4Q-sya",
        "outputId": "5500fd1e-3025-4014-ce39-a9cfad9f631c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's load the json-data\n",
        "\n",
        "file_sarc = open('/tmp/sarcasm.json', 'r')\n",
        "data = json.load(file_sarc)\n",
        "file_sarc.close()"
      ],
      "metadata": {
        "id": "WNDXMDMp_xbx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = data[0].keys()\n",
        "print(\"The 3 features associated with the news headlines are:\", *info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vshl8-KgAj6R",
        "outputId": "78316a18-9f6e-4ce5-bd1b-837eb5143dfe"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The 3 features associated with the news headlines are: article_link headline is_sarcastic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## let's create separate list of sentences, labels and  links/urls \n",
        "list_of_labels = list() ##sarcastic or non sarcastic\n",
        "list_of_links = list() ##url to the article \n",
        "list_of_sentences = list() \n",
        "\n",
        "for instance in data:\n",
        "  list_of_labels.append(instance['is_sarcastic'])\n",
        "  list_of_links.append(instance['article_link'])\n",
        "  list_of_sentences.append(instance['headline'])\n",
        "\n",
        "print(\"Length of each of these lists are:\",  len(list_of_labels), len(list_of_links), len(list_of_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU8hALWbBARC",
        "outputId": "b8beaace-69a4-4083-eb9c-ba79a36a81c7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of each of these lists are: 26709 26709 26709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's do  some standard preprocessing!!\n",
        "##create  a  tokenizer instance\n",
        "tokenizer_instance = TKN(oov_token = '<OOV>')\n",
        "##let's fit this object on the sentence lists\n",
        "tokenizer_instance.fit_on_texts(list_of_sentences)\n",
        "word_index = tokenizer_instance.word_index"
      ],
      "metadata": {
        "id": "gMGqXlA3CjUo"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index\n",
        "list_of_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLnSa53aIERx",
        "outputId": "dedc1a19-1f0c-4262-a2e4-6e67984ec36e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
              " \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
              " \"mom starting to fear son's web series closest thing she will have to grandchild\",\n",
              " 'boehner just wants wife to listen, not come up with alternative debt-reduction ideas',\n",
              " 'j.k. rowling wishes snape happy birthday in the most magical way']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer_instance.texts_to_sequences(list_of_sentences)\n",
        "##this tep was used to ensure tokens represent the words\n",
        "padded = padseq(sequences, padding='post') ##--> 'post' describes all the sequences will be padded to the longest sequence\n",
        "print(padded[0],'\\n', padded.shape) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVVvBcD1IF6h",
        "outputId": "6c47c54f-c814-4281-a924-60370a35bd82"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0] \n",
            " (26709, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Let's set the parameters\n",
        "size_vocab = 10000\n",
        "embedding_dimension = 16\n",
        "len_max = 32\n",
        "truncate = 'post'\n",
        "padding = 'post'\n",
        "ukn_token = '<OOV>'\n",
        "size_train = 20000"
      ],
      "metadata": {
        "id": "IVi1QTIVLrEu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's split into  train and tests!!\n",
        "xtrain, ytrain = list_of_sentences[0:size_train], list_of_labels[0:size_train]\n",
        "xtest,  ytest =  list_of_sentences[size_train:], list_of_labels[size_train:]"
      ],
      "metadata": {
        "id": "jl7sVBv4OnVf"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's tokenize and pad  our training and test data\n",
        "\n",
        "tokenizer = TKN(num_words=size_vocab, oov_token=ukn_token)\n",
        "tokenizer.fit_on_texts(xtrain)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(xtrain)\n",
        "train_pad = padseq(train_seq, maxlen=len_max, padding=padding, truncating=truncate)\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(xtest)\n",
        "test_pad = padseq(test_seq, maxlen=len_max, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "1PZ2CB9zPldq"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YQa9_9p_fSAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "b7wJttyxgp7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}